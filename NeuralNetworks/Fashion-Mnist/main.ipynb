{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('fashion-mnist_train.csv')\n",
    "df2=pd.read_csv('fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df1.iloc[0:, 0].values.reshape(-1, 1) #starting from 1st row because csv first row is column names\n",
    "y_test = df2.iloc[0:, 0].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df1.iloc[0:, 1:].values.T\n",
    "X_train = X_train.astype(float) / 255.0\n",
    "\n",
    "X_test = df2.iloc[0:, 1:].values.T\n",
    "X_test = X_test.astype(float) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # For Hidden Neurons\n",
    "        self.w1 = np.random.randn(hidden_size, input_size)*0.01\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "\n",
    "        # For Output Neurons\n",
    "        self.w2 = np.random.randn(output_size, hidden_size)*0.01\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "\n",
    "\n",
    "    def one_hot(self,Y):\n",
    "        \n",
    "        one_hot_Y = np.eye(10)[Y.flatten()]\n",
    "        one_hot_Y=one_hot_Y.T\n",
    "        return one_hot_Y\n",
    "\n",
    "\n",
    "    def activation(self,x,alpha=00.1): #tanh\n",
    "        # return np.tanh(x)\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def activation_deriv(selfx,alpha=00.1):\n",
    "        # tanh_x = self.activation(x)\n",
    "        # return 1 - tanh_x**2\n",
    "\n",
    "        # return x>0\n",
    "        \n",
    "        dx = np.ones_like(x)\n",
    "        dx[x < 0] = alpha\n",
    "        return dx\n",
    "        \n",
    "\n",
    "    def softmax(self, x):\n",
    "        ex = np.exp(x)  \n",
    "        return ex / np.sum(ex, axis=0)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.z1 = np.dot(self.w1, X) + self.b1 #Z corresponds to pre activation value\n",
    "        self.a1 = self.activation(self.z1) #a corresponds to activation value\n",
    "\n",
    "        self.z2 = np.dot(self.w2,self.a1 ) + self.b2 #Z corresponds to pre activation value\n",
    "\n",
    "        self.a2 = self.softmax(self.z2)#a corresponds to activation value\n",
    "        return self.a2\n",
    "\n",
    "    def compute_loss(self, y, y_pred):\n",
    "        m = y.shape[1]\n",
    "        cost = -(1/m)*np.sum(y*np.log(y_pred))\n",
    "        return cost\n",
    "\n",
    "    def backward(self, X, y_true, learning_rate):\n",
    "\n",
    "        m = X.shape[1]\n",
    "\n",
    "        self.dz2 = (self.a2 - y_true) #derivative of soft max \n",
    "        self.dw2 = (1/m)*np.dot(self.dz2,self.a1.T )\n",
    "        self.db2 = (1/m)*np.sum(self.dz2,axis=1,keepdims=True)\n",
    "\n",
    "\n",
    "        self.dz1 = (1/m) * np.dot(self.w2.T, self.dz2) * self.activation_deriv(self.z1) #derivative of activation\n",
    "        self.dw1 = (1/m)*np.dot(self.dz1,X.T )\n",
    "        self.db1 = (1/m)*np.sum(self.dz1,axis=1,keepdims=True)\n",
    "\n",
    "        self.w2 = self.w2 - learning_rate*self.dw2\n",
    "        self.b2 = self.b2 - learning_rate*self.db2\n",
    "        \n",
    "\n",
    "        self.w1 = self.w1 - learning_rate*self.dw1\n",
    "        self.b1 = self.b1 - learning_rate*self.db1\n",
    "\n",
    "\n",
    " \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        y=self.one_hot(y)\n",
    "        loss_list=[]\n",
    "        for epoch in range(epochs):\n",
    "            predictions = self.forward(X)\n",
    "            loss = self.compute_loss(y, predictions)\n",
    "            self.backward(X, y, learning_rate)\n",
    "            loss_list.append(loss)\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        plt.plot(range(1, len(loss_list) + 1), loss_list, marker='X')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title('Cost vs Epoch')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.306674038515013\n",
      "Epoch 1, Loss: 2.2800487657465363\n",
      "Epoch 2, Loss: 2.255329278479819\n",
      "Epoch 3, Loss: 2.2321315765677063\n",
      "Epoch 4, Loss: 2.210169626429108\n",
      "Epoch 5, Loss: 2.189228142444228\n",
      "Epoch 6, Loss: 2.169143631905517\n",
      "Epoch 7, Loss: 2.149790919694031\n",
      "Epoch 8, Loss: 2.131073412426159\n",
      "Epoch 9, Loss: 2.1129159761164726\n",
      "Epoch 10, Loss: 2.0952596757638706\n",
      "Epoch 11, Loss: 2.07805786142032\n"
     ]
    }
   ],
   "source": [
    "input_size = X_train.shape[0]\n",
    "hidden_size = 1000\n",
    "output_size = 10    \n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "nn.train(X_train, y_train, epochs=50,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.76%\n"
     ]
    }
   ],
   "source": [
    "test_predictions = nn.forward(X_test)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(test_predictions, axis=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.sum(predicted_labels.reshape(-1, 1) == y_test)\n",
    "accuracy = correct_predictions / len(predicted_labels.reshape(-1, 1))\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 35.91%\n"
     ]
    }
   ],
   "source": [
    "train_predictions = nn.forward(X_train)\n",
    "\n",
    "# Convert one-hot encoded predictions back to class labels\n",
    "predicted_labels = np.argmax(train_predictions, axis=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.sum(predicted_labels.reshape(-1, 1) == y_train)\n",
    "accuracy = correct_predictions / len(predicted_labels.reshape(-1, 1))\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
